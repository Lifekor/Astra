"""
–ú–æ–¥—É–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π –ê—Å—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
"""
import os
import glob
import json
from datetime import datetime
from intent_analyzer import IntentAnalyzer
from astra_mcp_memory import AstraMCPMemory

try:
    import tiktoken  # type: ignore
except ImportError:  # pragma: no cover - optional dependency
    tiktoken = None


def _count_tokens(text: str) -> int:
    """Approximate token count for a text fragment."""
    if tiktoken:
        enc = tiktoken.encoding_for_model("gpt-4o")
        return len(enc.encode(text))
    return len(text) // 4

class MemoryExtractor:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π"""
    
    def __init__(self, memory, api_key=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –ø–∞–º—è—Ç–∏
        
        Args:
            memory (AstraMemory): –û–±—ä–µ–∫—Ç –ø–∞–º—è—Ç–∏ –ê—Å—Ç—Ä—ã
            api_key (str, optional): API –∫–ª—é—á –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ OpenAI
        """
        self.memory = memory
        self.intent_analyzer = IntentAnalyzer(api_key)
        # Semantic memory backed by FAISS (falls back if dependencies missing)
        self.mcp_memory = AstraMCPMemory(data_dir=memory.get_file_path(""))

        # Debug log for tracing memory search steps
        self.debug_log_file = os.path.join("astra_data", "memory_debug.log")

        # –ö—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–Ω–µ–≤–Ω–∏–∫–æ–≤
        self.diaries = {}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–Ω–µ–≤–Ω–∏–∫–∏ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        self.load_diaries()

    def log_debug_step(self, step_name: str, data) -> None:
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–ª–∞–¥–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        try:
            if not hasattr(self, 'debug_log_file'):
                self.debug_log_file = os.path.join("astra_data", "memory_debug.log")

            entry = f"[{datetime.now().isoformat()}] {step_name}\n"
            if isinstance(data, (dict, list)):
                entry += json.dumps(data, ensure_ascii=False, indent=2)
            else:
                entry += str(data)
            entry += "\n" + "-" * 80 + "\n"

            with open(self.debug_log_file, "a", encoding="utf-8") as f:
                f.write(entry)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
    
    def load_diaries(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –¥–Ω–µ–≤–Ω–∏–∫–∏ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
        diary_path = self.memory.get_file_path("")  # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–π –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–∞–Ω–Ω—ã—Ö
        
        # –ò—â–µ–º –≤—Å–µ .txt —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–∞–Ω–Ω—ã—Ö
        diary_files = glob.glob(os.path.join(diary_path, "*.txt"))
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞–∂–¥—ã–π –¥–Ω–µ–≤–Ω–∏–∫
        for file_path in diary_files:
            diary_name = os.path.basename(file_path).replace(".txt", "")
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    diary_content = f.read()
                
                self.diaries[diary_name] = diary_content
                print(f"–ó–∞–≥—Ä—É–∂–µ–Ω –¥–Ω–µ–≤–Ω–∏–∫: {diary_name}")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–Ω–µ–≤–Ω–∏–∫–∞ {diary_name}: {e}")
    
    def get_memory_fragments(self, diary_name, chunk_size=500, overlap=100):
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–Ω–µ–≤–Ω–∏–∫ –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

        Args:
            diary_name (str): –ò–º—è –¥–Ω–µ–≤–Ω–∏–∫–∞
            chunk_size (int): –†–∞–∑–º–µ—Ä —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            overlap (int): –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤

        Returns:
            list: –°–ø–∏—Å–æ–∫ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–Ω–µ–≤–Ω–∏–∫–∞
        """
        if diary_name not in self.diaries:
            return []

        diary_content = self.diaries[diary_name]

        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å–º—ã—Å–ª–æ–≤—ã–º –±–ª–æ–∫–∞–º
        # 1. –°–Ω–∞—á–∞–ª–∞ –ø–æ –¥–≤–æ–π–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–∞–º (–∞–±–∑–∞—Ü—ã)
        paragraphs = [p.strip() for p in diary_content.split('\n\n') if p.strip()]

        fragments = []
        current_fragment = ""

        for paragraph in paragraphs:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç –ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞–±–∑–∞—Ü–∞ –ª–∏–º–∏—Ç
            if len(current_fragment) + len(paragraph) + 2 > chunk_size:
                if current_fragment:
                    fragments.append(current_fragment.strip())
                current_fragment = paragraph
            else:
                if current_fragment:
                    current_fragment += "\n\n" + paragraph
                else:
                    current_fragment = paragraph

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç
        if current_fragment:
            fragments.append(current_fragment.strip())

        # –ï—Å–ª–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ, –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö
        final_fragments = []
        i = 0
        while i < len(fragments):
            fragment = fragments[i]

            # –ü—ã—Ç–∞–µ–º—Å—è –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å —Å —Å–ª–µ–¥—É—é—â–∏–º–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º–∏
            while (
                i + 1 < len(fragments)
                and len(fragment) + len(fragments[i + 1]) + 2 < chunk_size
            ):
                fragment += "\n\n" + fragments[i + 1]
                i += 1

            final_fragments.append(fragment)
            i += 1

        return final_fragments

    def prefilter_fragments(self, fragments, query):
        """
        –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º

        Args:
            fragments (list): –°–ø–∏—Å–æ–∫ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
            query (str): –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å

        Returns:
            list: –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        """
        if not fragments:
            return []

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        query_words = query.lower().split()

        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
        synonyms = {
            '–¥–æ–º–∏–∫': ['–¥–æ–º', '–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å', '–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ', '–∫–æ–º–Ω–∞—Ç–∞', 'ui', '–º–∞–∫–µ—Ç', 'figma'],
            '—Å–æ–∑–¥–∞–Ω–∏–µ': ['—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ', '–æ–±—Å—É–∂–¥–µ–Ω–∏–µ', '—Å–æ–∑–¥–∞–≤–∞–ª', '–ø–æ—Å—Ç—Ä–æ–∏–ª'],
            '–ø—Ä–æ–≥—Ä–∞–º–º–∞': ['–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—Å–∏—Å—Ç–µ–º–∞', '–¥–æ–º', '–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞'],
            '–æ–±—Å—É–∂–¥–∞–ª–∏': ['–≥–æ–≤–æ—Ä–∏–ª–∏', '–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–ª–∏', '—Å–æ–∑–¥–∞–≤–∞–ª–∏', '–¥—É–º–∞–ª–∏', '—Ä–µ—à–∞–ª–∏']
        }

        # –†–∞—Å—à–∏—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
        expanded_words = set(query_words)
        for word in query_words:
            if word in synonyms:
                expanded_words.update(synonyms[word])

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        filtered_fragments = []
        for fragment in fragments:
            fragment_lower = fragment.lower()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            matches = sum(1 for word in expanded_words if word in fragment_lower)

            # –ò—Å–∫–ª—é—á–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ç–æ–ª—å–∫–æ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
            if matches > 0 and len(fragment) > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
                # –ò—Å–∫–ª—é—á–∞–µ–º —á–∏—Å—Ç—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                if not (fragment.startswith('üìî') and len(fragment) < 100):
                    filtered_fragments.append(fragment)

        # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å –º–∞–ª–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, –±–µ—Ä–µ–º –≤—Å–µ
        if len(filtered_fragments) < 5:
            return fragments

        return filtered_fragments
    
    def extract_relevant_memories(
        self,
        user_message,
        intent_data=None,
        conversation_context=None,
        model=None,
        memory_token_limit=3000,
    ):
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Args:
            user_message (str): –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            intent_data (dict, optional): –î–∞–Ω–Ω—ã–µ –æ –Ω–∞–º–µ—Ä–µ–Ω–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            conversation_context (list, optional): –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞
            model (str, optional): –ú–æ–¥–µ–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            memory_token_limit (int, optional): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–Ω–µ–≤–Ω–∏–∫–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
                –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            
        Returns:
            dict: –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è–º–∏
        """
        # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –æ –Ω–∞–º–µ—Ä–µ–Ω–∏–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã, –ø–æ–ª—É—á–∞–µ–º –∏—Ö
        if intent_data is None:
            intent_data = self.intent_analyzer.analyze_intent(
                user_message, conversation_context
            )
            self.log_debug_step("intent_analysis", intent_data)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø–∞–º—è—Ç–∏
        memory_types = intent_data.get("match_memory", [])

        # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø –∏–Ω—Ç–µ–Ω—Ç–∞
        intent = intent_data.get("intent", "")

        # –ü–æ–ø—ã—Ç–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ MCP –ø–∞–º—è—Ç–∏
        mcp_results = self.mcp_memory.semantic_search(user_message, top_k=5)
        if mcp_results:
            memories = [{"text": r["text"], "relevance": r["score"]} for r in mcp_results]
            sources = {r["text"]: r.get("source", "mcp") for r in mcp_results}
            return {"intent": intent, "memories": memories, "sources": sources}
        
        # –ú–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é gpt-3.5-turbo
        model_to_use = model or "gpt-3.5-turbo"

        self.log_debug_step(
            "memory_search_start",
            {
                "user_message": user_message,
                "memory_types": memory_types,
                "model": model_to_use,
                "token_limit": memory_token_limit,
            },
        )
        
        # –ï—Å–ª–∏ —Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç–æ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ —Ç–∏–ø—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞–º–µ—Ä–µ–Ω–∏—è
        if not memory_types:
            intent = intent_data.get("intent", "")
            
            if intent == "about_user":
                memory_types = ["relationship_memory", "user_preferences"]
            elif intent == "about_relationship":
                memory_types = ["relationship_memory", "astra_memories", "astra_intimacy"]
            elif intent == "about_astra":
                memory_types = ["core_memory", "astra_memories"]
            elif intent == "intimate":
                memory_types = ["astra_intimacy", "relationship_memory"]
            elif intent == "memory_recall":
                memory_types = ["astra_memories", "relationship_memory"]
            else:
                memory_types = ["astra_memories", "core_memory"]
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –≤—Å–µ—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø–∞–º—è—Ç–∏
        all_fragments = []
        fragments_sources = {}
        used_tokens = 0
        
        for memory_type in memory_types:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–∏–ø–∞ –ø–∞–º—è—Ç–∏ –≤ –∏–º—è —Ñ–∞–π–ª–∞
            diary_names = []
            
            if memory_type == "core_memory":
                diary_names = ["astra_core_prompt"]
            elif memory_type == "relationship_memory":
                diary_names = ["relationship_memory", "astra_memories"]
            elif memory_type == "emotion_memory":
                diary_names = ["emotion_memory", "tone_memory", "subtone_memory", "flavor_memory"]
            elif memory_type == "user_preferences":
                diary_names = ["relationship_memory", "user_preferences"]
            else:
                # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –∏–º—è –∫–∞–∫ –∏–º—è –¥–Ω–µ–≤–Ω–∏–∫–∞
                diary_names = [memory_type]
            
            # –ü–æ–ª—É—á–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –∫–∞–∂–¥–æ–≥–æ –¥–Ω–µ–≤–Ω–∏–∫–∞
            for diary_name in diary_names:
                # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ self.diaries
                diary_key = diary_name.replace(".txt", "")
                
                if diary_key in self.diaries and used_tokens < memory_token_limit:
                    fragments = self.get_memory_fragments(diary_key)

                    for fragment in fragments:
                        tokens = _count_tokens(fragment)
                        if used_tokens + tokens > memory_token_limit:
                            break
                        all_fragments.append(fragment)
                        fragments_sources[fragment] = diary_name
                        used_tokens += tokens
                    if used_tokens >= memory_token_limit:
                        break
            if used_tokens >= memory_token_limit:
                break
        # –ï—Å–ª–∏ –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ

        self.log_debug_step(
            "fragments_collected",
            {
                "count": len(all_fragments),
                "used_tokens": used_tokens,
                "sources": list({v for v in fragments_sources.values()}),
            },
        )

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
        if all_fragments:
            all_fragments = self.prefilter_fragments(all_fragments, user_message)
            self.log_debug_step(
                "prefiltered_fragments",
                {
                    "count": len(all_fragments),
                    "sample": all_fragments[:2] if all_fragments else [],
                },
            )
        
        # –ï—Å–ª–∏ —É –Ω–∞—Å –Ω–µ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if not all_fragments:
            return {
                "intent": intent_data.get("intent", "unknown"),
                "memories": [],
                "sources": {}
            }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É—è –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        relevance_data = self.intent_analyzer.get_semantic_relevance(
            user_message,
            all_fragments,
            model=model_to_use,
            intent=intent,
            strategy="compact_relevance",
            memory_token_limit=800,
        )
        relevant_fragments = relevance_data.get("fragments", [])
        self.log_debug_step("relevance_result", relevance_data)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            "intent": intent_data.get("intent", "unknown"),
            "memories": relevant_fragments,
            "sources": {},
        }

        if "_token_usage" in relevance_data:
            result["_token_usage"] = relevance_data["_token_usage"]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
        for fragment in relevant_fragments:
            text = fragment["text"]
            if text in fragments_sources:
                result["sources"][text] = fragments_sources[text]

        self.log_debug_step("memory_search_result", result)

        return result
